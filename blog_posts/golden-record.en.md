---
title: "Golden Record"
description: "Serverless redshift ETL"
employer: "XalDigital"
thumbnail: "https://cdn.prod.website-files.com/67c3a771f50ee86278e081b8/67cbf293dbee0c171ba3e2cf_67cbde6602d17199fe025917_black%2520belt%2520badge.png"
banner: "https://cdn.prod.website-files.com/67c3a771f50ee86278e081b8/67cbe41e022e968fe92ed459_67cbdfe83b32df846e2a44a4_photo-1732320935426-395f3c1d38be.jpeg"
startDate: "Sat Feb 04 2023 06:00:00 GMT+0000 (Coordinated Universal Time)"
endDate: "Mon May 20 2024 05:00:00 GMT+0000 (Coordinated Universal Time)"
lucidchartUrl: ""
tags:
  - name: "Python"
    color: "hsla(90, 50%, 50%, 0.3)"
  - name: "AWS Glue"
    color: "hsla(30, 100%, 50%, 0.3)"
  - name: "AWS Redshift"
    color: "hsla(0, 50%, 50%, 0.3)"
  - name: "AWS CloudFormation"
    color: "hsla(210, 50%, 40%, 0.3)"
  - name: "AWS Step Functions"
    color: "hsla(120, 100%, 50%, 0.3)"
  - name: "Project Planning"
    color: "hsla(210, 100%, 40%, 0.3)"
  - name: "ETL Pipelines"
    color: "hsla(331, 83%, 49%, 0.3)"
  - name: "Amazon CloudWatch"
    color: "hsla(240, 100%, 40%, 0.3)"
  - name: "AWS SNS"
    color: "hsla(30, 100%, 50%, 0.3)"
---

üöÄ **Automating Data Flows with SAS and AWS**

I Worked on a project where **SAS** running on an internal server generates CSV files and uploads them to the **AWS** cloud for processing.

Data goes through a **Landing Zone** and then to a **Model Zone**

where **AWS Glue** launches queries and **AWS Step Functions** orchestrates the entire process üîÑ

In the end, the information is integrated into **Redshift** and transferred to SAS via **Lambda** and notified with SNS when it is ready

‚Äç

‚Äç

The best thing about this architecture is how it connects internal systems (Rackspace and Aerom√©xico) with the AWS cloud without interrupting existing flows, and also allows scaling and automating data delivery
