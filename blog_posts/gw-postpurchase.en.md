---
title: "GW PostPurchase"
description: "Serverless redshift ETL"
employer: "XalDigital"
thumbnail: ""
banner: "https://cdn.prod.website-files.com/67c3a771f50ee86278e081b8/67cbe41e022e968fe92ed459_67cbdfe83b32df846e2a44a4_photo-1732320935426-395f3c1d38be.jpeg"
startDate: "Mon Jun 12 2023 05:00:00 GMT+0000 (Coordinated Universal Time)"
endDate: "Fri Feb 02 2024 06:00:00 GMT+0000 (Coordinated Universal Time)"
lucidchartUrl: ""
tags:
  - name: "Python"
    color: "hsla(90, 50%, 50%, 0.3)"
  - name: "AWS Glue"
    color: "hsla(30, 100%, 50%, 0.3)"
  - name: "AWS Redshift"
    color: "hsla(0, 50%, 50%, 0.3)"
  - name: "AWS CloudFormation"
    color: "hsla(210, 50%, 40%, 0.3)"
  - name: "AWS Step Functions"
    color: "hsla(120, 100%, 50%, 0.3)"
  - name: "Project Planning"
    color: "hsla(210, 100%, 40%, 0.3)"
  - name: "ETL Pipelines"
    color: "hsla(331, 83%, 49%, 0.3)"
  - name: "Amazon CloudWatch"
    color: "hsla(240, 100%, 40%, 0.3)"
  - name: "AWS SNS"
    color: "hsla(30, 100%, 50%, 0.3)"
---

üöÄ **Automating Data Flows with SAS and AWS**
I recently worked on a project where **SAS** running on an internal server generates CSV files and uploads them to the **AWS** cloud for processing.

Data goes through a **Landing Zone** and then to a **Model Zone**

where **AWS Glue** launches queries and **AWS Step Functions** orchestrates the entire process üîÑ

In the end, the information is integrated into **Redshift** and transferred to SAS via **Lambda** and notified with SNS when it is ready

‚Äç

‚Äç

‚Äç

‚Äç

‚Äç

‚Äç

The best thing about this architecture is how it connects internal systems (Rackspace and Aerom√©xico) with the AWS cloud without interrupting existing flows, and also allows scaling and automating data delivery

‚Äç
